---
title: Consistency is the Key to Further Mitigating Catastrophic Forgetting in Continual
  Learning
abstract: Deep neural networks struggle to continually learn multiple sequential tasks
  due to catastrophic forgetting of previously learned tasks. Rehearsal-based methods
  which explicitly store previous task samples in the buffer and interleave them with
  the current task samples have proven to be the most effective in mitigating forgetting.
  However, Experience Replay (ER) does not perform well under low-buffer regimes and
  longer task sequences as its performance is commensurate with the buffer size. Consistency
  in predictions of soft-targets can assist ER in preserving information pertaining
  to previous tasks better as soft-targets capture the rich similarity structure of
  the data. Therefore, we examine the role of consistency regularization in ER framework
  under various continual learning scenarios. We also propose to cast consistency
  regularization as a self-supervised pretext task thereby enabling the use of a wide
  variety of self-supervised learning methods as regularizers. While simultaneously
  enhancing model calibration and robustness to natural corruptions, regularizing
  consistency in predictions results in lesser forgetting across all continual learning
  scenarios. Among the different families of regularizers, we find that stricter consistency
  constraints preserve previous task information in ER better.
video: https://youtu.be/Hqag-gq8JU4
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bhat22b
month: 0
tex_title: Consistency is the Key to Further Mitigating Catastrophic Forgetting in
  Continual Learning
firstpage: 1195
lastpage: 1212
page: 1195-1212
order: 1195
cycles: false
bibtex_author: Bhat, Prashant Shivaram and Zonooz, Bahram and Arani, Elahe
author:
- given: Prashant Shivaram
  family: Bhat
- given: Bahram
  family: Zonooz
- given: Elahe
  family: Arani
date: 2022-11-28
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 28
pdf: https://proceedings.mlr.press/v199/bhat22b/bhat22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
