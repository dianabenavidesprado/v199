---
title: Test Sample Accuracy Scales with Training Sample Density in Neural Networks
abstract: Intuitively, one would expect accuracy of a trained neural network’s prediction
  on test samples to correlate with how densely the samples are surrounded by seen
  training samples in representation space. We find that a bound on empirical training
  error smoothed across linear activation regions scales inversely with training sample
  density in representation space. Empirically, we verify this bound is a strong predictor
  of the inaccuracy of the network’s prediction on test samples. For unseen test sets,
  including those with out-of-distribution samples, ranking test samples by their
  local region’s error bound and discarding samples with the highest bounds raises
  prediction accuracy by up to 20% in absolute terms for image classification datasets,
  on average over thresholds.
video: https://youtu.be/GF_-LcdudXI
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ji22a
month: 0
tex_title: Test Sample Accuracy Scales with Training Sample Density in Neural Networks
firstpage: 629
lastpage: 646
page: 629-646
order: 629
cycles: false
bibtex_author: Ji, Xu and Pascanu, Razvan and Hjelm, R. Devon and Lakshminarayanan,
  Balaji and Vedaldi, Andrea
author:
- given: Xu
  family: Ji
- given: Razvan
  family: Pascanu
- given: R. Devon
  family: Hjelm
- given: Balaji
  family: Lakshminarayanan
- given: Andrea
  family: Vedaldi
date: 2022-11-28
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 28
pdf: https://proceedings.mlr.press/v199/ji22a/ji22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
