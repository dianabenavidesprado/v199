---
title: Inherent Limitations of Multi-Task Fair Representations
abstract: With the growing awareness to fairness in machine learning and the realization
  of the central role that data representation has in data processing tasks, there
  is an obvious interest in notions of fair data representations. The goal of such
  representations is that a model trained on data under the representation (e.g.,
  a classifier) will be guaranteed to respect some fairness constraints, while still
  being expressive enough to model the task well. Such representations are useful
  when they can be fixed for training models on various different tasks and also when
  they serve as data filtering between the raw data (available to the representation
  designer) and potentially malicious agents that use the data under the representation
  to learn predictive models and make decisions. A long list of recent research papers
  strive to provide tools for achieving these goals. However, we prove that in most
  cases, such goals are inaccessible! Roughly stated, we prove that no representation
  can guarantee the fairness of classifiers for different tasks trained using it (while
  retaining the needed expressive powers). The reasons for this impossibility depend
  on the notion of fairness one aims to achieve. For the basic ground-truth-independent
  notion of Demographic (or Statistical) Parity, the obstacle is conceptual; a representation
  that guarantees such fairness inevitably depends on the marginal (unlabeled) distribution
  of the relevant instances, and in most cases that distribution changes from one
  task to another. For more refined notions of fairness, that depend on some ground
  truth classification, like Equalized Odds (requiring equality of error rates between
  groups), fairness cannot be guaranteed by a representation that does not take into
  account the task specific labeling rule with respect to which such fairness will
  be evaluated (even if the marginal data distribution is known a priori). Furthermore,
  for tasks sharing the same marginal distribution, we prove that except for trivial
  cases, no representation can guarantee Equalized Odds fairness for any two different
  tasks while enabling accurate label predictions for both.
video: https://youtu.be/r_nrLxrPQrY
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lechner22a
month: 0
tex_title: Inherent Limitations of Multi-Task Fair Representations
firstpage: 583
lastpage: 603
page: 583-603
order: 583
cycles: false
bibtex_author: Lechner, Tosca and Ben-David, Shai
author:
- given: Tosca
  family: Lechner
- given: Shai
  family: Ben-David
date: 2022-11-28
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 28
pdf: https://proceedings.mlr.press/v199/lechner22a/lechner22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
