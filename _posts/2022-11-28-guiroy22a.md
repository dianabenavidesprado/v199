---
title: Improving Meta-Learning Generalization with Activation-Based Early-Stopping
abstract: Meta-learning algorithms for few-shot learning aim to train neural networks
  capable of generalizing to novel tasks using only a few examples. Early-stopping
  is critical for performance, halting model training when it reaches optimal generalization
  to the new task distribution. Early-stopping mechanisms in Meta-Learning typically
  rely on measuring the model performance on labeled examples from a meta-validation
  set drawn from the training (source) dataset. This is problematic in few-shot transfer
  learning settings, where the meta-test set comes from a different target dataset
  (OOD) and can potentially have a large distributional shift with the meta-validation
  set. In this work, we propose Activation Based Early-stopping (ABE), an alternative
  to using validation-based early-stopping for meta-learning. Specifically, we analyze
  the evolution, during meta-training, of the neural activations at each hidden layer,
  on a small set of unlabelled support examples from a single task of the target tasks
  distribution, as this constitutes a minimal and justifiably accessible information
  from the target problem. Our experiments show that simple, label agnostic statistics
  on the activations offer an effective way to estimate how the target generalization
  evolves over time. At each hidden layer, we characterize the activation distributions,
  from their first and second order moments, then further summarized along the feature
  dimensions, resulting in a compact yet intuitive characterization in a four-dimensional
  space. Detecting when, throughout training time, and at which layer, the target
  activation trajectory diverges from the activation trajectory of the source data,
  allows us to perform early-stopping and improve generalization in a large array
  of few-shot transfer learning settings, across different algorithms, source and
  target datasets.
video: https://youtu.be/C4py5DYHRwA
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guiroy22a
month: 0
tex_title: Improving Meta-Learning Generalization with Activation-Based Early-Stopping
firstpage: 213
lastpage: 230
page: 213-230
order: 213
cycles: false
bibtex_author: Guiroy, Simon and Pal, Christopher and Mordido, Goncalo and Chandar,
  Sarath
author:
- given: Simon
  family: Guiroy
- given: Christopher
  family: Pal
- given: Goncalo
  family: Mordido
- given: Sarath
  family: Chandar
date: 2022-11-28
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 28
pdf: https://proceedings.mlr.press/v199/guiroy22a/guiroy22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
