---
title: 'Adapting Pre-trained Language Models to Low-Resource Text Simplification:
  The Path Matters'
abstract: 'We frame the problem of text simplification from a task and domain adaptation
  perspective, where neural language models are pre-trained on large-scale corpora
  and then adapted to new tasks in different domains through limited training examples.
  We investigate the performance of two popular vehicles of task and domain adaptation:
  meta-learning and transfer learning (in particular fine-tuning), in the context
  of low-resource text simplification that involves a diversity of tasks and domains.
  We find that when directly adapting a Web-scale pre-trained language model to low-resource
  text simplification tasks, fine-tuning based methods present a competitive advantage
  over meta-learning approaches. Surprisingly, adding an intermediate stop in the
  adaptation path between the source and target, an auxiliary dataset and task that
  allow for the decomposition of the adaptation process into multiple steps, significantly
  increases the performance of the target task. The performance is however sensitive
  to the selection and ordering of the adaptation strategy (task adaptation vs. domain
  adaptation) in the two steps. When such an intermediate dataset is not available,
  one can build a pseudostop using the target domain/task itself. Our extensive analysis
  serves as a preliminary step towards bridging these two popular paradigms of few-shot
  adaptive learning and towards developing more structured solutions to task/domain
  adaptation in a novel setting.'
video: https://youtu.be/8uKBL7Om1JY
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: garbacea22a
month: 0
tex_title: 'Adapting Pre-trained Language Models to Low-Resource Text Simplification:
  The Path Matters'
firstpage: 1103
lastpage: 1119
page: 1103-1119
order: 1103
cycles: false
bibtex_author: Garbacea, Cristina and Mei, Qiaozhu
author:
- given: Cristina
  family: Garbacea
- given: Qiaozhu
  family: Mei
date: 2022-11-28
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 28
pdf: https://proceedings.mlr.press/v199/garbacea22a/garbacea22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
