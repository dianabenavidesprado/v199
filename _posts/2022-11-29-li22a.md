---
title: Energy-Based Models for Continual Learning
abstract: We motivate Energy-Based Models (EBMs) as a promising model class for continual
  learning problems. Instead of tackling continual learning via the use of external
  memory, growing models, or regularization, EBMs change the underlying training objective
  to cause less interference with previously learned information. Our proposed version
  of EBMs for continual learning is simple, efficient, and outperforms baseline methods
  by a large margin on several benchmarks. Moreover, our proposed contrastive divergence-based
  training objective can be combined with other continual learning methods, resulting
  in substantial boosts in their performance. We further show that EBMs are adaptable
  to a more general continual learning setting where the data distribution changes
  without the notion of explicitly delineated tasks. These observations point towards
  EBMs as a useful building block for future continual learning methods.
video: https://youtu.be/iZ4cz5RQmzE
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22a
month: 0
tex_title: Energy-Based Models for Continual Learning
firstpage: 1
lastpage: 22
page: 1-22
order: 1
cycles: false
bibtex_author: Li, Shuang and Du, Yilun and van de Ven, Gido and Mordatch, Igor
author:
- given: Shuang
  family: Li
- given: Yilun
  family: Du
- given: Gido
  family: Ven
  prefix: van de
- given: Igor
  family: Mordatch
date: 2022-11-29
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 29
pdf: https://proceedings.mlr.press/v199/li22a/li22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
