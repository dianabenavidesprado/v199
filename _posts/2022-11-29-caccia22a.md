---
title: On Anytime Learning at Macroscale
abstract: 'In many practical applications of machine learning data arrives sequentially
  over time in large chunks. Practitioners have then to decide how to allocate their
  computational budget in order to obtain the best performance at any point in time.
  Online learning theory for convex optimization suggests that the best strategy is
  to use data as soon as it arrives. However, this might not be the best strategy
  when using deep non-linear networks, particularly when these perform multiple passes
  over each chunk of data rendering the overall distribution non i.i.d.. In this paper,
  we formalize this learning setting in the simplest scenario in which each data chunk
  is drawn from the same underlying distribution, and make a first attempt at empirically
  answering the following questions: How long should the learner wait before training
  on the newly arrived chunks? What architecture should the learner adopt? Should
  the learner increase capacity over time as more data is observed? We probe this
  learning setting using convolutional neural networks trained on classic computer
  vision benchmarks as well as a large transformer model trained on a large-scale
  language modeling task. Code is available in the supplementary material.'
video: https://youtu.be/UbaFAhVSFJw
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: caccia22a
month: 0
tex_title: On Anytime Learning at Macroscale
firstpage: 165
lastpage: 182
page: 165-182
order: 165
cycles: false
bibtex_author: Caccia, Lucas and Xu, Jing and Ott, Myle and Ranzato, Marcaurelio and
  Denoyer, Ludovic
author:
- given: Lucas
  family: Caccia
- given: Jing
  family: Xu
- given: Myle
  family: Ott
- given: Marcaurelio
  family: Ranzato
- given: Ludovic
  family: Denoyer
date: 2022-11-29
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 29
pdf: https://proceedings.mlr.press/v199/caccia22a/caccia22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
